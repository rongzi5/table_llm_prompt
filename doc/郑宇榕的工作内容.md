# 郑宇榕的工作内容

我是郑宇榕，我在项目中负责的工作是通过本地部署大模型，并调用来批量处理数据集的问题，并通过提示词工程进一步优化模型在数据集上的表现。

## Zero Shot

我首先使用何锦诚编写的 Zero-shot 的系统提示词 [zero-shot-prompt.js](https://github.com/JacksonHe04/smart-table-llm/blob/main/llm-api/prompts/zero-shot-prompt.js)：

```
You are an accurate table Q&A assistant. Please carefully analyze the following table and answer the questions accurately. Note: Only output the answers, do not explain the process.
"table_text":table content;
"statement": question.
Please give the answer directly.
```

在代码中是：

```python
SYSTEM_PROMPT = (
"""You are an accurate table Q&A assistant. Please carefully analyze the following table and answer the questions accurately. Note: Only output the answers, do not explain the process.
"table_text":table content;
"statement": question.
Please give the answer directly.
"""
)
```

我（郑宇榕）、何锦诚、李凯文基于测试集的同一个随机子集[test_100.jsonl](https://github.com/JacksonHe04/smart-table-llm/blob/main/datasets/test_100.jsonl)分别展开了测试。


## Qwen2.5-7B-Instruct

## Zero Shot

我将该提示词应用到 Qwen2.5-7B-Instruct 模型上，ACC 是47.7%。

详细的测试结果见[system_zero-shot](https://github.com/rongzi5/table_llm_prompt/tree/2f9c01add08272e77709c9854427d4222bb1455b/system_zero-shot)。

## <span id="oneshot"> One Shot</span>

受到[fetaqa/prompt.py](https://github.com/TIGER-AI-Lab/TableCoT/blob/a9071776576c9e6047e8194aaf1c67fc2369c3b4/fetaqa/prompt.py) 的启发，我将其中的例子作为one-shot的示例，在代码中表现为
```python
demonstration = """
Example:
Table_text:
[["Year", "Title", "Role", "Channel"],
["2015", "Kuch Toh Hai Tere Mere Darmiyaan", "Sanjana Kapoor", "Star Plus"],
["2016", "Kuch Rang Pyar Ke Aise Bhi", "Khushi", "Sony TV"],
["2016", "Gangaa", "Aashi Jhaa", "&TV"],
["2017", "Iss Pyaar Ko Kya Naam Doon 3", "Meghna Narayan Vashishth", "Star Plus"],
["2017–18", "Tu Aashiqui", "Richa Dhanrajgir", "Colors TV"],
["2019", "Laal Ishq", "Pernia", "&TV"],
["2019", "Vikram Betaal Ki Rahasya Gatha", "Rukmani/Kashi", "&TV"],
["2019", "Shaadi Ke Siyape", "Dua", "&TV"]]
Statement: What TV shows was Shagun Sharma seen in 2019?
Answer: Laal Ishq, Vikram Betaal Ki Rahasya Gatha, Shaadi Ke Siyape"""
```

同样在 Qwen2.5-7B-Instruct 上进行了独立的 5 次测试，每次随机抽取 100 个案例，ACC 的平均值是 49.6%。

测试结果记录见 [one-shot](https://github.com/rongzi5/table_llm_prompt/tree/2f9c01add08272e77709c9854427d4222bb1455b/one-shot)。

## <span id="html">html</span>
受到论文["Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study"](https://dl.acm.org/doi/10.1145/3616855.3635752)的启发，在这里我尝试将表格的输入形式转换成为html格式，在 Qwen2.5-7B-Instruct 上进行了独立的 5 次测试，每次随机抽取 100 个案例，ACC 的平均值是 50.6%。

## <span id="one-shot+html">one-shot + html</span>
在此处我尝试将html格式运用到one-shot当中，将one-shot的例子转换成html形式一起输入。在 Qwen2.5-7B-Instruct 上进行了独立的 5 次测试，每次随机抽取 100 个案例，ACC 的平均值是 48.7%。
此处ACC值反而没有[one-shot](#oneshot)和[html](#html)高，可能的原因有：
1. 上下文窗口限制：同时使用两种方法会占用更多token，可能压缩了其他有用信息的空间。由于Qwen2.5-7B-Instruct模型参数较小，处理长上下文时性能会下降。

## self-augmented
由于[one-shot + html](#one-shot+html)的结果不理想，最终我仍然采用原格式输入表格。我尝试了LLM自增强方法：先让模型自动总结表格内容，再将总结文本附加到原问题中。这种方法显著提升了模型对表格内容的理解能力。
相关代码表示为:
```python
SA = ("""In no more than five sentences, extract the most critical information from the table below, including:
1. For numeric columns, the key numeric ranges or extreme values;
2. For string/categorical columns, the top 2–3 most frequent categories;
3. The names of the columns and a brief note on what each represents.
Here is the table:""")

SA_prompt = SA + str(item.get("table", ""))

SA_resp = llm.chat(
[[{"role": "system", "content": SYSTEM_PROMPT},
{"role": "user", "content": SA_prompt}]],
sampling_params=sampling_params
)[0].outputs[0].text.strip()

augmented = (
"Extracted critical information:"
f"{SA_resp}\n"
"Table:"
f"{str(item.get('table', ''))}\n"
"statement:"
f"{item.get('question', '')}"
)

final_resp = llm.chat(
[[{"role": "system", "content": SYSTEM_PROMPT},
{"role": "user", "content": augmented}]],
sampling_params=sampling_params
)[0].outputs[0].text.strip()
```
在 Qwen2.5-7B-Instruct 上进行了独立的 5 次测试，每次随机抽取 100 个案例，ACC 的平均值是 51.6%。相关测试结果可见[system+SA](https://github.com/rongzi5/table_llm_prompt/tree/2f9c01add08272e77709c9854427d4222bb1455b/system%2BSA)

## 测试结果表格
<!-- TODO -->
| 模型 | 方法 | ACC | 测试样本数 | 测试次数 | 备注 |
|------|------------|-----|------------|----------|------|
| Qwen2.5-7B-Instruct | Zero-shot | 47.7% | 100 | 5 |  |
| Qwen2.5-7B-Instruct | One-shot | 49.6% | 100 | 5 |  |
| Qwen2.5-7B-Instruct | html | 50.6% | 100 | 5 | |
| Qwen2.5-7B-Instruct | One-shot+html | 48.7% | 100 | 5 | 性能下降 |
| Qwen2.5-7B-Instruct | self-augmented | 51.6% | 100 | 5 | |

# References

```bash
@article{sui2023table,
  title     = {Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study},
  author    = {Yuan Sui and Mengyu Zhou and Mingjie Zhou and Shi Han and Dongmei Zhang},
  journal   = {Web Search and Data Mining},
  year      = {2023},
  doi       = {10.1145/3616855.3635752},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f534f566535f4e0fd2b72b1db3b18c47479e5092}
